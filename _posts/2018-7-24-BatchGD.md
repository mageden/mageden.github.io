---
layout: post
title: "Introduction to Gradient Descent"
tags: Optimization
use_math: true
---

* TOC
{:toc}

## Introduction

Gradient descent is an optimization algorithm which finds either the minimum (gradient descent) or maximum (gradient ascent) of some specified function by taking a step proportional to the gradient (a vector of partial derivatives) along the gradient. Gradient descent is widely used in machine learning in order to estimate model parameters which minimize some cost function, such as the sum of squares or deviance. A useful analogy for how the algorithm works is imagining hiking along a mountain pass and trying to find the lowest point in heavy fog. We can't see the whole pass and need to instead just use the information right next to us. A reasonable strategy would be to look at the slope where we are standing, and walk some distance along the steepest slope. At we get closer to the bottom the slope will flatten out, until we reach the lowest point, where the slope will be 0. Gradient descent uses this methodology, with the gradient being the slope where we are standing. In this case we have a function with two inputs (longitude and latitude), but the method generalizes to any number of inputs.

More precisely, the algorithm can be broken down into the following steps;

1. Choose stepping size $\lambda$ ($\lambda \in (0,1)$): Controls how far along the gradient each iteration moves
2. Choose starting values $X_0$ in $F(X)$
3. Calculate the gradient at $\frac{\partial}{\partial X}F(X_0)$
4. Calculate $X_1 = X_0 - \lambda \frac{\partial}{\partial X}F(X_0)$ (or + for ascent)
5. Continue until N iterations or until the change in F(X) between iterations is smaller than some specified tolerance $\tau$

<img src="/images/posts/BatchGD/simple_illustration.png" width="100%" style="padding:0px"/>

Gradient descent is at heart a simple algorithm, however, there are many variations that are used to address different challenges with the cost surface. This post aims to introduce some optimization concepts as well as some simple examples of batch gradient descent with a static learning parameter (the details will be explained below). Later posts will get into other types of gradient descent with adaptive learning parameters and different training sizes (mini-batch, stochastic).

### Convex Functions

It is important when optimizing a function to know whether it is strictly concave/convex or if it is not. Convex functions have only one minimum (one maximum for concave functions), making optimization easier as we don't have to worry about the optimization algorithm getting stuck in a local extremum. As an example, below are two functions, on convex and one non-convex. The convex function can be seen to have only one minimum across its range, while the nonconvex function has a local minimum in addition to its global minimum.  

<img src="/images/posts/BatchGD/convex_nonconvex.png" width="100%" style="padding:0px"/>

Local minimum can be problematic with batch gradient descent as there is no way to distinguish a local minimum from a global minimum when using the gradient (both have a partial derivative of 0). Local minimum consequently make the algorithm sensitive to starting values in respect to the final solution. Variations of gradient descent that deal with local minimum, such as stochastic gradient descent, will be covered in a later post.

Another situation that arises in non-convex functions are saddle points, where there is a plateau in the center of a region in which some dimensions go down and others go up (in 2-D this looks like a saddle). It can be easy for batch gradient descent to get stuck in the center of the saddle as the gradient is 0 in all directions. 

### Learning Rate

A key hyperparameter in gradient descent is the learning rate $\tau$, which determines the proportion of the gradient that we move along with each iteration. Tuning the learning rate can be challenging as too small of a value and the algorithm takes longer to converge, but too large of a value and it may never converge. In the simplest form of gradient descent used here, the learning rate is a static value that remains constant across all iterations. The example problems below will show how adjusting the learning parameter influences convergence.

<img src="/images/posts/BatchGD/learning_rate.png" width="100%" style="padding:0px"/>

There are more advanced methods that adjust the learning rate across iterations in order to speed up convergence, using either learning rate schedules or adaptive learning rates, which are used in almost all real world implementations. This post just covers the simplest case in order to teach the basics of how the algorithm works.

### Stopping Conditions

When using any optimization algorithm the user needs to define some stopping conditions. The simplest choice is to arbitrarily choose N iterations and then plot the change in cost function over time in order to see if the estimates have converged towards a particular solution. If the algorithm has not converged by the last iteration then it can be run again starting at the final value of the previous run. There are a few limitations of this method. Visually deciding when the algorithm is 'close enough' is not a very rigorous assessment of convergence and makes replication across analysts challenging. The selection of the number of iterations is also problematic, as too low of a value and the algorithm doesn't converge, too high and computational power was wasted.

A second method for choosing the number of iterations is to use the concept of tolerance, or the amount of change allowed between iterations. In other words, on each iteration we evaluate the change in some metric of our function, and we stop the algorithm on the first iteration for which the change is less than the specified tolerance $\tau$. This method allows for a more rigorous treatment of convergence by defining what is close enough, but ill-conditioned problems or improper parameters may cause the algorithm to be stuck or take excessively long trying to reach $\tau$. For this reason we often specify both the maximum number of iterations for the algorithm to run as well as a tolerance, as this allows us to limit the maximum running time while still specifying a convergence criterion.

With this concept in mind there are three primary ways tolerance can be defined in gradient descent; cost tolerance, where we evaluate the change in the cost funciton, gradient tolerance, where we evaluate the gradient, and step tolerance, where we evaluate the change in input.

Cost tolerance is likely the most common type of tolerance used in gradient descent, and has a conveniently simple form. Tolerance is specified relative to the magnitude of the cost function  in order to control for different scales of the cost function. If $J_i = J(X_{i,1},...,X_{i,k})$ is our cost function, then it can be represented as;

$$
|J_{i+1}-J_i| < \tau (|J_{i}|+\tau)
$$

### Batch Size

Gradient descent in the context of machine learning and statistics usually involves selecting a lack of fit function of the data, and then minimizing this function. When all of the observations are used on each iteration of the algorithm it is called batch gradient descent, as the entire batch of data is used. There is no requirement, however, that the entire batch of data is used for each iteration; instead, we can update our estimates after each observation (stochastic gradient descent) or after smaller batches of observation (mini-batch). This can help the algorithm escape local minima by the additional noise added to the optimization process, as well as speed up convergence.

### Standardizing Input

When running any model one often runs into the advice of standardizing the variables (subtracting the mean and dividing by the standard deviation). One of the reasons for this is that non-standardized variables with wildly different scales can have a negative impact on the speed of convergence for optimization algorithms. When standardized the cost surface is circular, while when non-standardized the cost surface can develop 'troughs' where there is a large space with relatively small differences in many of the parameters. For example, look at these two contour plots which show the mean squared error (our cost function) for a linear regression with two parameters; an intercept and slope for $X$. The plot on the left has standardized $X$, while the plot on the right has not. The non-standardized contour plot shows larger ellipses with a larger amount of space that has to be traversed to find the minimum relative to the standardized surface.

<img src="/images/posts/BatchGD/standardized.png" width="100%" style="padding:0px"/>

## Examples

In order to get a better feel for how gradient descent works I have worked through two examples; one using a simple linear regression and one using a non-convex function called the Rosenbrock function.

### Linear Regression

Batch gradient descent was used, wherein each iteration of the algorithm used all of the data. A standard linear regression with n=100 was simulated according to the following model;

$$
\begin{align*}
x_i & \sim N(0,1) \\
\epsilon_i & \sim N(0,1) \\
y_i & = 30 + 30x_i + \epsilon_i \\
\end{align*}
$$

One choice for the cost function could be the sum of squares, however, this would make the cost surface sensitive to the sample size. Mean squared error (MSE) addresses this minor problem with little additional computational cost and results in the same final estimate.

$$
\begin{align*}
Cost(\beta,y) = MSE(\beta_0,\beta_1,...,\beta_k) & =\frac{1}{N}\sum_{i=1}^N(y_i-\hat{y_i})^2 \\
\frac{\partial MSE(\beta_0,...,\beta_k)}{\partial \beta_0} & =-\frac{2}{N}\sum_{i=1}^N(y_i-\hat{y_i}) \\
\frac{\partial MSE(\beta_0,...,\beta_k)}{\partial \beta_1} & =-\frac{2}{N}\sum_{i=1}^N x_{1,i}(y_i-\hat{y_i}) \\
\end{align*}
$$

The initialization values were $\beta_0 = -40$ and $\beta_1 = -40$ with $\tau = 10^{-8}$.

<img src="/images/posts/BatchGD/linear_regression.png" width="100%" style="padding:0px"/>

The first stepping size of $\lambda = .01$ appeared too small, so a second model was run with a larger $\lambda = .5$ with much more rapid convergence.

<img src="/images/posts/BatchGD/lr_cost.png" width="100%" style="padding:0px"/>

### Rosenbrock Function

The Rosenbrock function, sometimes called the banana function, is often used when testing optimization algorithms as it is nonconvex and has a large valley that is difficult to quickly traverse. It is easy to get close to the minimum of the Rosenbrock function, but challenging to reach the true minimum. Too large of values for $\lambda$ result in NA results as the algorithm jumps around to the steep walls and balloons out of control.

$$f(x_1,x_2)=(a-x_1)^2+b(x_2-{x_1}^2)^2$$

$$
\begin{align*}
\frac{\partial f(x_1,x_2)}{\partial x_1} & =4x_1^3b-4bx_1 x_2+2x_1-2a \\
\frac{\partial f(x_1,x_2)}{\partial x_2} & =2b(x_2-x_1^2) \\
\end{align*}
$$

Here I used $a=1$ and $b=100$ so that the global minimum was located at $x_1=1$ and $x_2=1$. The maximum iterations was 100000 and $\tau=10^{-8}$ with starting values of -40 , -40.

<img src="/images/posts/BatchGD/rosenbrock.png" width="100%" style="padding:0px"/>

When increasing the stepping size a fairly small amount the algorithm gets stuck, and with a larger stepping size the estimates balloon out of control.

<img src="/images/posts/BatchGD/rosenbrock_cost.png" width="100%" style="padding:0px"/>

## Appendix: Code

I wrote new functions in order to better learn how it works. This code is not optimized and is instead intended as a learning tool.

### Linear Regression

```r
LinearRegression_gradient <- function(y,X,
                                      init = NULL,
                                      iter=10000,
                                      stepping =  0.01,
                                      tolerance = .0000001,
                                      history = FALSE,
                                      silent = FALSE) {
  N = length(y)
  if(!is.matrix(X)){X=as.matrix(X)}
  design.matrix = cbind(Intercept=1,X)
  number.coef = ncol(design.matrix)
  # Cost
  CalcCost = function(y,design.matrix,coef, N){
    sum((y - design.matrix %*% coef)^2)/N
  }
  cost_history <- rep(NA, iter)
  # History
  if(history){coef_history <- matrix(rep(NA,number.coef*iter),
                                     nrow=iter)}
  # coefficient setup
  if(!is.null(init)) {
    coef <- matrix(init, nrow=number.coef)
  } else {
    coef <- matrix(rep(0,number.coef),nrow=number.coef)
  }
  rownames(coef) <- colnames(design.matrix)
  reached.tolerance = FALSE
  # Initialize
  cost_history[1] <- CalcCost(y,design.matrix,coef,N)
  coef_history[1,] <- coef[1:number.coef]
  i = 2
  # implement algorightm
  while(i<=iter & !reached.tolerance) {
    error <- (y-design.matrix %*% coef)
    gradient <- -t(design.matrix) %*% error / (N/2)
    coef[1:number.coef]<-coef-stepping*gradient
    cost_history[i]=CalcCost(y,design.matrix,coef,N)
    if(history){coef_history[i,]=coef[1:number.coef]}
    if(i>1) {
      cost.change = abs(cost_history[i]-cost_history[i-1])
      tolerance.condition = tolerance*(abs(cost_history[i-1])+tolerance)
      reached.tolerance = ifelse(cost.change<tolerance.condition,TRUE, FALSE)
    }
    i = i + 1
  }
  # Remove empty rows if needed
  if(i!=(iter+1)){
    coef_history <- coef_history[rowSums(is.na(coef_history))==0,]
    cost_history <- cost_history[!is.na(cost_history)]
  }
  # Structure output
  output <- list()
  output$coef <- t(coef)
  if(history){output$coef_history <- coef_history}
  output$cost <- cost_history
  output$iter <- nrow(coef_history)
  if(!silent){eval(print(output$coef),parent.frame())}
  return(invisible(output))
}
```

### Rosenbrock Function

```r
Rosenbrock_gradient <- function(a=1, b=100,
                                init = NULL,
                                iter = 10000,
                                stepping =  0.001,
                                tolerance = .0000001,
                                silent = FALSE) {
  # History
  coef_history <- matrix(rep(NA,2*iter), nrow=iter)
  colnames(coef_history)=c("x1","x2")
  # coefficient setup
  if(!is.null(init)) {
    coef <- init
  } else {
    coef <- rnorm(2)
  }
  names(coef) <- c("x1","x2")
  # Rosenbrock stuff
  rosenbrock <- function(x1,x2,a,b) {
    (a-x1)^2 + b*(x2-x1^2)^2
  }
  cost_history <- rep(NA, iter)
  gradient_x1 <- function(x1,x2,a,b) {
    4*b*x1^3 + 2*x1 - 4*x1*b*x2 - 2*a
  }
  gradient_x2 <- function(x1,x2,a,b){
    2*b*(x2-x1^2)
  }
  gradient <- c(NA,NA)
  # Setup
  coef_history[1,] = coef[1:2]
  cost_history[1] = rosenbrock(coef[1],coef[2],a,b)
  i = 2
  reached.tolerance = FALSE
  # implement algorightm
  while(i<=iter & !reached.tolerance) {
    gradient[1] <- gradient_x1(coef[1],coef[2],a,b)
    gradient[2] <- gradient_x2(coef[1],coef[2],a,b)
    coef<-coef-stepping*gradient
    cost_history[i] <- rosenbrock(coef[1],coef[2],a,b)
    coef_history[i,]=coef[1:2]
    if(i>1) {
      cost.change = abs(cost_history[i]-cost_history[i-1])
      tolerance.condition = tolerance*(abs(cost_history[i-1])+tolerance)
      reached.tolerance = ifelse(cost.change<tolerance.condition,TRUE, FALSE)
    }
    i = i+1
  }
  if(i!=(iter+1)) {
    coef_history <- coef_history[rowSums(is.na(coef_history))==0,]
    cost_history <- cost_history[!is.na(cost_history)]
  }
  output <- list()
  output$coef <- t(coef)
  output$cost <- cost_history
  output$iter <- nrow(coef_history)
  output$coef_history <- coef_history
  if(!silent){eval(print(output$coef),parent.frame())}
  return(invisible(output))
}
```
